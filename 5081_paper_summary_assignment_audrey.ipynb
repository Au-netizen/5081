{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links for chosen paper: \n",
    "\n",
    "https://www.nature.com/articles/s41467-020-14496-6\n",
    "Supplementary data: https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-020-14496-6/MediaObjects/41467_2020_14496_MOESM1_ESM.pdf\n",
    "Peer review manuscript: https://static-content.springer.com/esm/art%3A10.1038%2Fs41467-020-14496-6/MediaObjects/41467_2020_14496_MOESM2_ESM.pdf\n",
    "\n",
    "I would hope that we don't need to have citations for this assignment, let alone of other articles..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper is uh pretty cool and within my field. The topic is insect (by the end more specifically bees and bumblebees) research and in particular how well they do and are present in urban and rural sites in a temperate forest global north landscape. My research project is on pollinator (by the end more specifically bees) research and in particular how well they do and are present in roadside plant patches in a temperate forest global north landscape. \n",
    "\n",
    "It has among other things, a structural equation model in it, you told me I should look into structural equation models so here I am. It is indeed a published research paper from a peer-reviewed journal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Methods summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental design:\n",
    "\n",
    "The base of this experiment is a set of 9 sites in urban areas and 9 sites in rural areas across central-Eastern Germany in the format of a paired study design. They were selected based on being sufficiently flower-rich and functionally similar, representative of landscape ecology in central Europe. These sites already had a fair amount of floral abundance and diversity based on the descriptions provided in the article, however the portion of the landscape which was analyzed in particular were 10 potted Trifolium pratense plants with 8 marked open infloresences in each study site. They were randomly distributed across 1x10 meters of the overall 25x25 meter plots of the overall study sites. These were present at the study site for 5 days, which was also the amount of days which sampling occured per location. It is worth noting that for each respective urban/rural pairing, the sampling was done at the same dates.\n",
    "\n",
    "At these sites on those 5 days, diversity and abundance of insects was measured amongst the following taxa: Dipetra, Lepidopetra, Coleoptera, and in particular Hymenoptera. Pan traps were used to collect the specimens, and barcoding techniques were used to categorize species richness.\n",
    "\n",
    "Effectively/in summary, a variety of variables are used to try and find trends in insect visitation and abundance, and how large of a seed set aka eventual number of seeds from the potted plants are developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of variables related to Trifolium pratense seed set:\n",
    "\n",
    "Set 1: rural/urban site as predictor variable, edge density and proportion of green land as dependent variables\n",
    "Set 2: edge density and proportion of green land as predictor variables, Hymenoptera OTU (operational toxonomic units) as dependent variable\n",
    "Set 3: edge density and proportion of green land as predictor variables, insect visitation rate is dependent variable\n",
    "Set 4: local flower richness/abundance as predictor variables, Hymenopetra OTU richness as dependent variable\n",
    "Set 5: Hymenopetra OTU richness as predictor variable, Trifolium pratense seed set aka eventual number of seeds as dependent variable\n",
    "Set 6: local flower richness/abundance as predictor variables, Trifolium pratense seed set as dependent variable\n",
    "Set 7: insect visitation rate as predictor variable, Trifolium pratense seed set as dependent variable\n",
    "Set 8: Conspecific density (numnber of same species in an area) of flowers as predictor variable, Trifolium pratense seed set as dependent variable\n",
    "Set 9: Hymenoptera phylogenetic species variabilit (PSV) as predictor variable, Trifolium pratense seed set as dependent variable\n",
    "Set 10: rural/urban site as predictor variable, Trifolium pratense seed set as dependent variable\n",
    "\n",
    "List of variables related to Trifolium pratense visitation rates:\n",
    "\n",
    "Set 1: rural/urban site as predictor variable, Bombus spp and Apis mellifera and other insect vistation rate per 30 minutes as dependent variables\n",
    "\n",
    "List of variables related to plant flower composition of study sites:\n",
    "Set 1: rural/urban site as predictor variable, flower richness and flower abundance and number of inflorescences of co-flowering Trifolium pratense plants as dependent variables\n",
    "\n",
    "Additional note: there is also variables utilized in this study for species identification, but I see it as past the scope of this assignment and they are not used in any of my simulations, so I will not go too in depth here. But some examples include mean nearest taxon distance and saturation of OTU richness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical tests used:\n",
    "\n",
    "Mixed model analyses:\n",
    "\n",
    "Linear mixed model\n",
    "Generalized linear model\n",
    "Generalized linear mixed model\n",
    "Structural equation model\n",
    "\n",
    "Variance inflation factors were used to check regression models\n",
    "\n",
    "Additional note: there is also statistical tests utilized in this study for species identification, but I see it as past the scope of this assignment and they are not used in any of my simulations, so I will not go too in depth here. But some examples include calculations for mean nearest taxon distance and saturation of OTU richness, as well as all the statistical methods which go into DNA barcoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical methods summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be real I do not think I am nearly qualified enough to comment on whether these statistical methods were appropriate for this study and their research questions. I can try and explain why, but even then I have doubts over my competence here. \n",
    "\n",
    "I think that these linear models were the were the simplest statistical analyses they could use that would still provide them somewhat robust analysis, and a good rule of thumb to follow for models is that the simplest practically available model which can be utilized \n",
    "\n",
    "Having their results effectively be tied to controlled potted Trifolium pratense plants gave a lot more consistency to their data, so simpler linear models work out fine(?) here. They had VIFs of less than 3 in every instance , which according to uh some probably bigoted dead white guy means that given ideal conditions (again look back to the condition of controlled Trifolium pratense) there is low correlation among variables, which is good for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How authors intrepreted the statistical results:\n",
    "\n",
    "They thought it was good? I feel like as a scientist, if your experimental setup is conventionally categorized as robust enough to work with simpler linear models, and your linear models give outcomes which have pretty clear ways of describing them with biological terms, you can be quite happy with that regardless of if any given statistical test returned significant results or not. If there's not really an association to be found between a set of variables, that's cool, there is more than enough tests around here.\n",
    "\n",
    "The authors moreorless* reported their findings as is, and got to the overall conclusion that bees did well in various metrics across urban sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data simulation and analysis via Python 3.12.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figures chosen to simulate:\n",
    "\n",
    "1. Supplementary figure 1: Proportion of green land uses and edge density in the 9 urban and 9 rural sites\n",
    "2. Main figure 5: Trifolium visitation rates by varying insect species in 9 urban and 9 rural sites\n",
    "3. Main figure 2: Mean species richness/otu at 9 urban and 9 rural sites\n",
    "4. Main figure 4: Mean number of seed set for Trifolium in sites in the 9 urban and 9 rural sites\n",
    "5. Main figure 7: Structural equation model chart of Trifolium pretense seed set\n",
    "\n",
    "This gives the context for what datasets exactly I even want to simulate in the latter sections of this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data simulation: \n",
    "This section has both data simulation but also direct replication or attempts of replication of specific data points and associations, because I could do it (explained why this is beneficial later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# Subsequent chunks of code will have these repeated, but basically this is a \n",
    "# general list of python libraries (they're called libraries in python and \n",
    "# packages in R idk why) all in one location\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import networkx as nxy\n",
    "import numpy as npy\n",
    "import semopy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Data for 9 urban and 9 rural sites and their proportion of green land\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "\n",
    "# So uh I am creating a data library. You said it should all be simulated using\n",
    "# R (or in my case Python), so I am inputting the values for the dataset here\n",
    "# rather than importing a text, csv, xlsx etc file. For now I am just inputting\n",
    "# building cover percentage, where in the other component of the 100% is the\n",
    "# green space which is the relevant variable. Basically, in their data building\n",
    "# cover was more easily put together as one column of data so I am starting with\n",
    "# the single percentage of that first rather than adding some up for a green\n",
    "# space percentage to save myself some time.\n",
    "data1 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_Potsdam', 'R_Berlin', 'R_Gottingen'\n",
    "    ],\n",
    "    'building_cover_percentage': [\n",
    "        71.3536, 58.0860, 71.6511, 31.2379, 32.8523, 87.8783, 15.0954, 56.1380, \n",
    "        94.6886, 10.0467, 5.5687, 0.0, 1.8657, 3.4797, 0.4461, 3.8421, 9.6161, \n",
    "        1.1836\n",
    "    ]\n",
    "}\n",
    "\n",
    "# First plot of the data we have, for the entire green space chunk of data here\n",
    "# it is just generated by a little subtraction operation, preeetty cool\n",
    "plots_data1 = pdy.DataFrame(data1)\n",
    "plots_data1['green_space_percentage'] = \\\n",
    "    100 - plots_data1['building_cover_percentage']\n",
    "print(plots_data1)\n",
    "\n",
    "# Making a starting plot for now... which I in hindsight didn't really need for\n",
    "# this section of the assignment. Will not have any other plots until the data\n",
    "# analysis simulation. Yes I use hexcodes for the colour of my figures and imo\n",
    "# anyone that doesn't is a little silly when it comes to this one specific topic\n",
    "mpy.figure(figsize=(12, 6))\n",
    "mpy.bar(plots_data1['site_name'], plots_data1['green_space_percentage'], \n",
    "        color='#599b19', label='Green Space %')\n",
    "mpy.bar(plots_data1['site_name'], plots_data1['building_cover_percentage'], \n",
    "        bottom=plots_data1['green_space_percentage'], color='#bec1bc', \n",
    "        label='Building Cover %')\n",
    "mpy.title('Building Cover and Green Space Percentages')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Percentage')\n",
    "mpy.xticks(rotation=45)\n",
    "mpy.ylim(0, 100)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n",
    "\n",
    "# Weird thing to keep note of: I think because 'building_cover_percentage' \n",
    "# alphabetically starts before 'green_space_percentage', there needs to be \n",
    "# explicit code choosing which is the bottom component. Without this command, \n",
    "# just switching the around the order of the two lines of code which are \n",
    "# actually visualizing the bar will not cause this basically I wanted \n",
    "# 'green_space_percentage' at the bottom and it was annoying to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Seperately making dataset of the 18 study sites' plots and strips\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "\n",
    "# This separate chunk of code assigning numeric values to each of these lablled\n",
    "# variables is needed if I want to fit them in with the site name data points. \n",
    "# It has to be this way. Well not really you could just do the 25 * 25 within\n",
    "# the lists of the data1a = {} code but that is bad habits, because with more\n",
    "# complicated data points it is better to be able to refer to past variables you\n",
    "# have made sure were assigned a mathematical equation or some statistical model\n",
    "# or anything else ahead of time.\n",
    "core_plot_area_sqmeters = 25 * 25\n",
    "potted_plant_length_meters = 10\n",
    "potted_plant_width_meters = 1\n",
    "potted_plant_area_sqmeters = potted_plant_length_meters \\\n",
    "    * potted_plant_width_meters\n",
    "\n",
    "\n",
    "data1a = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_Potsdam', 'R_Berlin', 'R_Gottingen'\n",
    "    ],\n",
    "    'core_plot_area_sqmeters': [\n",
    "        core_plot_area_sqmeters\n",
    "    ] * 18,\n",
    "    'potted_plant_length_meters': [\n",
    "        potted_plant_length_meters\n",
    "    ] * 18,\n",
    "    'potted_plant_width_meters': [\n",
    "        potted_plant_width_meters\n",
    "    ] * 18,\n",
    "    'potted_plant_area_sqmeters': [\n",
    "        potted_plant_area_sqmeters\n",
    "    ] * 18\n",
    "}\n",
    "\n",
    "\n",
    "plots_data1a = pdy.DataFrame(data1a)\n",
    "\n",
    "plots_data1a['core_plot_area_sqmeters'] = core_plot_area_sqmeters\n",
    "plots_data1a['potted_plant_area_sqmeters'] = potted_plant_area_sqmeters\n",
    "\n",
    "print(plots_data1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Combining the two above\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "\n",
    "data1 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_Potsdam', 'R_Berlin', 'R_Gottingen'\n",
    "    ],\n",
    "    'building_cover_percentage': [\n",
    "        71.3536, 58.0860, 71.6511, 31.2379, 32.8523, 87.8783, 15.0954, 56.1380, \n",
    "        94.6886, 10.0467, 5.5687, 0.0, 1.8657, 3.4797, 0.4461, 3.8421, 9.6161, \n",
    "        1.1836\n",
    "    ]\n",
    "}\n",
    "\n",
    "plots_data1 = pdy.DataFrame(data1)\n",
    "\n",
    "plots_data1['green_space_percentage'] = 100 - \\\n",
    "    plots_data1['building_cover_percentage']\n",
    "\n",
    "core_plot_area_sqmeters = 25 * 25  \n",
    "potted_plant_length_meters = 10  \n",
    "potted_plant_width_meters = 1  \n",
    "potted_plant_area_sqmeters = potted_plant_length_meters \\\n",
    "    * potted_plant_width_meters  \n",
    "\n",
    "plots_data1['core_plot_area_sqmeters'] = core_plot_area  \n",
    "plots_data1['potted_plant_area_sqmeters'] = potted_plant_area  \n",
    "\n",
    "print(plots_data1)\n",
    "\n",
    "# Got rid of the figure part of code here, it's pretty redundant now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Adding bombus visitations as a data set, same for OTU richness and seed set\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "\n",
    "# Not too much to explain here, the code that was used to get building cover\n",
    "# percentage data to match with the sites data points is now being used for\n",
    "# bombus visitations, OTU richness of insects, and T pretense seed set.\n",
    "data1 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_Potsdam', 'R_Berlin', 'R_Gottingen'\n",
    "    ],\n",
    "    'building_cover_percentage': [\n",
    "        71.3536, 58.0860, 71.6511, 31.2379, 32.8523, 87.8783, 15.0954, 56.1380, \n",
    "        94.6886, 10.0467, 5.5687, 0.0, 1.8657, 3.4797, 0.4461, 3.8421, 9.6161, \n",
    "        1.1836\n",
    "    ],\n",
    "    'bombus_visitations': [\n",
    "        3, 31, 4, 22, 11, 4, 0, 2, 13, 2, 0, 2, 1, 1, 1, 0, 0, 17\n",
    "    ],\n",
    "    'OTU_richness': [\n",
    "        80, 93, 68, 53, 57, 73, 61, 71, 62, 76, 122, 105, 90, 77, 81, 74, 71, 83\n",
    "    ],\n",
    "    'seed_set': [307, 276, 260, 212, 198, 181, 159, 114, 334, 287, 223, 210,\n",
    "    208, 201, 138, 118, 89, 36\n",
    "        \n",
    "    ]\n",
    "}\n",
    "# For some reason, they decided to not give the raw seed set data for each site\n",
    "# after the 5-day sampling period let alone after every day... weird. Anyways I\n",
    "# had to get a screenshot of figure 4 in the main article cropped to only\n",
    "# include the height of the bar from the y-axis of 0 to 350, then do pixel\n",
    "# count. In this case the image was 658 pixels tall, so for every mouse hover of\n",
    "# one of the 18 points in figure 4 I do a mouse hover on an image editing app,\n",
    "# use that new pixel coordinate and do proportinate calculations. Very fun\n",
    "# The authors also do not exactly make it clear in parts of their supplementary\n",
    "# exactly what insect taxa they are referring to with \"OTU richness\", grrr\n",
    "\n",
    "plots_data1 = pdy.DataFrame(data1)\n",
    "\n",
    "plots_data1['green_space_percentage'] = 100 \\\n",
    "    - plots_data1['building_cover_percentage']\n",
    "\n",
    "\n",
    "print(plots_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on rationale of the data simulation:\n",
    "\n",
    "Based on what you were saying about the purpose of this assignment, my interpretation was that I do not need to be super exhuastive about what figures I am reverse engineering and replicating, so I can handpick some. I went along with that suggestion because I like working less when possible. There have been 5 different figures or set of figures chosen for this assignment, but even within that I was selective of which data points I replicated. In effect, my data analysis simulation will not look the same as what is in the base article. A large part of this is my condition of trying to have the raw data available for me as well, or at least the best I could anyways. So in this way I am making extra work for myself of also doing code for raw data sets (and their subsequent analysis), but I think this improves my learning so I went with it. It was a bit tedious (especially the pixel counting stuff) but I think I did indeed learn more in depth for myself this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Actually simulating data now!\n",
    "# This first simulation starting here is based on how I would expect simulation \n",
    "# to go when just kinda visually perceiving the trends of the data/figures. The \n",
    "# second simulation is directly based off the numbers I found in my data set. \n",
    "# The third is again the approach used of what I would expect just kinda\n",
    "# perceiving the trends but with better prespective now if that makes sense.\n",
    "\n",
    "import numpy as npy \n",
    "import pandas as pdy\n",
    "\n",
    "# Definining parameters, I know it's a 9 there for now it will get repeated\n",
    "numb_sites = 9\n",
    "radius = 1000\n",
    "\n",
    "#Seed to replicate (for now...)\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Land percentages based on how the rural green space was sliiightly higher\n",
    "# Note that the variables are different now\n",
    "urban_green_space_percentage2 = npy.random.uniform(0.1, 0.4, numb_sites)\n",
    "rural_green_space_percentage2 = npy.random.uniform(0.5, 0.9, numb_sites)\n",
    "\n",
    "urban_building_cover_percentage2 = 1 - urban_green_space_percentage2\n",
    "rural_building_cover_percentage2 = 1 - rural_green_space_percentage2\n",
    "\n",
    "# Repeated code from the data replication sections, just added a 2\n",
    "core_plot_area_sqmeters2 = 25 * 25  \n",
    "potted_plant_length_meters2 = 10  \n",
    "potted_plant_width_meters2 = 1  \n",
    "potted_plant_area_sqmeters2 = potted_plant_length_meters2 \\\n",
    "    * potted_plant_width_meters2\n",
    "\n",
    "# A couple of things to go over for these variables. Overall they repeat a\n",
    "# similar process of simulation as for the green space percentages, but there is\n",
    "# a bit of extra code to make sure they are rounded numbers.\n",
    "bombus_visitations2 = npy.concatenate((\n",
    "    npy.random.normal(8, 1, numb_sites).round().astype(int),\n",
    "    npy.random.normal(2.5, 0.5, numb_sites).round().astype(int)\n",
    "))\n",
    "\n",
    "OTU_richness2 = npy.concatenate((\n",
    "    npy.random.normal(68, 5, numb_sites).round().astype(int),\n",
    "    npy.random.normal(87, 5, numb_sites).round().astype(int)\n",
    "))\n",
    "\n",
    "bee_OTU_richness = npy.concatenate((\n",
    "    npy.random.normal(10, 2, numb_sites).round().astype(int),\n",
    "    npy.random.normal(6, 1, numb_sites).round().astype(int)\n",
    "))\n",
    "\n",
    "seed_set2 = npy.concatenate((\n",
    "    npy.random.normal(230, 20, numb_sites).round().astype(int),\n",
    "    npy.random.normal(168, 15, numb_sites).round().astype(int)\n",
    "))\n",
    "# This specific formatting/spacing is the only way I could get this chunk of\n",
    "# code simulating the above categories to fit into my 80 character ruler, and it\n",
    "# thankfully ended up looking very clean :)\n",
    "\n",
    "# Data library again, but it's simulated ooh\n",
    "data2 = {\n",
    "    'Site': [f'Urban_{i+1}' for i in range(numb_sites)] + \\\n",
    "        [f'Rural_{i+1}' for i in range(numb_sites)],\n",
    "    'Type': ['Urban'] * numb_sites + ['Rural'] * numb_sites,\n",
    "    'green_space_percentage2': npy.concatenate((urban_green_space_percentage2, \\\n",
    "        rural_green_space_percentage2)),\n",
    "    'building_cover_percentage2': \\\n",
    "        npy.concatenate((urban_building_cover_percentage2, \\\n",
    "            rural_building_cover_percentage2)),\n",
    "    'core_plot_area_sqmeters2': [core_plot_area_sqmeters2] * (numb_sites * 2),\n",
    "    'potted_plant_area_sqmeters2': [potted_plant_area_sqmeters2] * \\\n",
    "        (numb_sites * 2),\n",
    "    'bombus_visitations2': bombus_visitations2,\n",
    "    'OTU_richness2': OTU_richness2,\n",
    "    'bee_OTU_richness': bee_OTU_richness,\n",
    "    'seed_set2': seed_set2\n",
    "}\n",
    "\n",
    "df = pdy.DataFrame(data2)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Simulating data based directly on the data replication I made in python\n",
    "\n",
    "# First going to directly have the chunks of code for data replication in here.abs\n",
    "# Technically this is not necessary as in a jupyter notebook file the python\n",
    "# code I ran previously aka the previous variables are still stored here. But it\n",
    "# doesn't hurt for reference and cases like you open this file and only want to\n",
    "# run this line of simulation instead of everything, it is easier to have it in\n",
    "# one code cell. Can seperately name stuff with data3 as well.\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "data3 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ],\n",
    "    'building_cover_percentage': [\n",
    "        71.3536, 58.0860, 71.6511, 31.2379, 32.8523, 87.8783, 15.0954, 56.1380, \n",
    "        94.6886, 10.0467, 5.5687, 0.0, 1.8657, 3.4797, 0.4461, 3.8421, 9.6161, \n",
    "        1.1836\n",
    "    ],\n",
    "    'bombus_visitations': [\n",
    "        3, 31, 4, 22, 11, 4, 0, 2, 13, 2, 0, 2, 1, 1, 1, 0, 0, 17\n",
    "    ],\n",
    "    'OTU_richness': [\n",
    "        80, 93, 68, 53, 57, 73, 61, 71, 62, 76, 122, 105, 90, 77, 81, 74, 71, 83\n",
    "    ],\n",
    "    'seed_set': [307, 276, 260, 212, 198, 181, 159, 114, 334, 287, 223, 210,\n",
    "                 208, 201, 138, 118, 89, 36]\n",
    "}\n",
    "plots_data3 = pdy.DataFrame(data3)\n",
    "plots_data3['green_space_percentage'] = 100 - \\\n",
    "plots_data3['building_cover_percentage']\n",
    "print(plots_data3)\n",
    "\n",
    "# Setting seed again, this turned out to be quite useful\n",
    "npy.random.seed(14)\n",
    "\n",
    "# In hindsight this sort of division of urban and rural categories should have\n",
    "# always been something I had in my code even in the replicated data section of\n",
    "# this submission, but I guess having access to \"raw data\" can make one turn a\n",
    "# blind eye to what is necessary for robust statistical programming. Something\n",
    "# to keep in mind.\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "# This is the actual substance of the simulation. With the current way that this\n",
    "# cell of code for data3 is set up, print(plots_combined_data) is still showing\n",
    "# the explicitly listed numbers because of what specific variables are listed in\n",
    "# 'combined_data'. The last line of code for this cell is the quick fix for it,\n",
    "# and is thus the appropriate simulated data chart which shows after* the \n",
    "# figures, but yeah I still think having bot is helpful.\n",
    "urban_building_cover = npy.random.normal(loc = 42, scale = 30, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "urban_bombus_visitations = npy.random.normal(loc = 8, scale = 2, \\\n",
    "     size = urban_count).round().astype(int)\n",
    "urban_OTU_richness = npy.random.normal(loc = 67, scale = 10, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "urban_seed_set = npy.random.normal(loc = 230, scale = 30, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_building_cover = npy.random.normal(loc = 45, scale = 100, \\\n",
    "    size=rural_count).round().astype(int)\n",
    "rural_bombus_visitations = npy.random.normal(loc = 3, scale = 0.5, \\\n",
    "    size=rural_count).round().astype(int)\n",
    "rural_OTU_richness = npy.random.normal(loc = 87, scale = 13, \\\n",
    "    size=rural_count).round().astype(int)\n",
    "rural_seed_set = npy.random.normal(loc = 163, scale = 30, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "# This section of the code in my assignment made it really clear why the\n",
    "# findings for the proportion of green land-usage were not significant, cool!\n",
    "\n",
    "# This is just needed to make sure there are not any data points (especially for\n",
    "# proportion of green space/building cover for rural sites). And yes it has to\n",
    "# be in this specific order, I thought it would be before the normal\n",
    "# distribution simulations but apparently numpy doesn't work like that.\n",
    "urban_building_cover = npy.clip(urban_building_cover, a_min=0, a_max=100)\n",
    "rural_building_cover = npy.clip(rural_building_cover, a_min=0, a_max=100)\n",
    "\n",
    "# This is the thing I was tallking about, it is pretty redundant?\n",
    "combined_data = {\n",
    "    'site_name': data3['site_name'],\n",
    "    'building_cover_percentage': npy.concatenate([urban_building_cover, \\\n",
    "        rural_building_cover]),\n",
    "    'bombus_visitations': npy.concatenate([urban_bombus_visitations, \\\n",
    "        rural_bombus_visitations]),\n",
    "    'OTU_richness': npy.concatenate([urban_OTU_richness, rural_OTU_richness]),\n",
    "    'seed_set': npy.concatenate([urban_seed_set, rural_seed_set])\n",
    "}\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "plots_combined_data['green_space_percentage'] = 100 - \\\n",
    "    plots_combined_data['building_cover_percentage']\n",
    "print(plots_combined_data)\n",
    "\n",
    "# This is new I uh think. Likewise I honestly still cannot tell if it is really\n",
    "# functionally necessary or if there is another way to get effectively the same\n",
    "# simulation outcome with less clutter.\n",
    "plot_data3 = plots_combined_data.set_index('site_name').reset_index()\n",
    "plot_data3['type'] = npy.where(plot_data3['site_name'].str.startswith('U'), \\\n",
    "    'Urban', 'Rural')\n",
    "\n",
    "# List of metrics to plot\n",
    "metrics = ['green_space_percentage', 'bombus_visitations', 'OTU_richness', \\\n",
    "    'seed_set']\n",
    "\n",
    "# Making the plot again, pretty boring\n",
    "for metric in metrics:\n",
    "    mpy.figure(figsize=(12, 6))\n",
    "    mpy.bar(plot_data['site_name'], plot_data[metric], \\\n",
    "        color=['#bec1bc' if t == \\\n",
    "        'Urban' else '#599b19' for t in plot_data['type']])\n",
    "    \n",
    "    mpy.title(f'Comparison of {metric.replace(\"_\", \" \").title()} by Site')\n",
    "    mpy.xlabel('Site Name')\n",
    "    mpy.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    mpy.xticks(rotation=45)\n",
    "    mpy.tight_layout()\n",
    "    \n",
    "    mpy.show()\n",
    "\n",
    "print(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Simulating data by taking upsides of previous 2 routes\n",
    "# It's mostly using techniques in data3 without the listed replicated data\n",
    "# points present, but bee_OTU_richness is also present\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "urban_bee_OTU_richness = npy.random.normal(loc = 9, scale = 4, \\\n",
    "    size= urban_count).round().astype(int)\n",
    "urban_building_cover = npy.random.normal(loc = 42, scale = 30, \\\n",
    "    size= urban_count).round().astype(int)\n",
    "urban_bombus_visitations = npy.random.normal(loc = 8, scale = 2, \\\n",
    "    size= urban_count).round().astype(int)\n",
    "urban_OTU_richness = npy.random.normal(loc = 67, scale = 10, \\\n",
    "    size= urban_count).round().astype(int)\n",
    "urban_seed_set = npy.random.normal(loc = 230, scale = 30, \\\n",
    "    size= urban_count).round().astype(int)\n",
    "rural_bee_OTU_richness = npy.random.normal(loc = 5.5, scale = 3, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "rural_building_cover = npy.random.normal(loc = 45, scale = 100, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "rural_bombus_visitations = npy.random.normal(loc = 3, scale = 0.5, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "rural_OTU_richness = npy.random.normal(loc = 87, scale = 13, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "rural_seed_set = npy.random.normal(loc = 163, scale = 30, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "# It doesn't hurt to be more exhuastive with these npy.clip functions here from\n",
    "# what I understand.\n",
    "urban_bee_OTU_richness = npy.clip(urban_bee_OTU_richness, a_min = 0, \\\n",
    "    a_max = None)\n",
    "rural_bee_OTU_richness = npy.clip(rural_bee_OTU_richness, a_min = 0, \\\n",
    "    a_max = None)\n",
    "urban_bombus_visitations = npy.clip(urban_bombus_visitations, a_min = 0, \\\n",
    "    a_max = None)\n",
    "rural_bombus_visitations = npy.clip(rural_bombus_visitations, a_min = 0, \\\n",
    "    a_max = None)\n",
    "urban_building_cover = npy.clip(urban_building_cover, a_min = 0, a_max = 100)\n",
    "rural_building_cover = npy.clip(rural_building_cover, a_min = 0, a_max = 100)\n",
    "urban_seed_set = npy.clip(urban_seed_set, a_min = 0, a_max = None)\n",
    "rural_seed_set = npy.clip(rural_seed_set, a_min = 0, a_max = None)\n",
    "urban_OTU_richness = npy.clip(urban_OTU_richness, a_min = 0, a_max = None)\n",
    "rural_OTU_richness = npy.clip(rural_OTU_richness, a_min = 0, a_max = None)\n",
    "\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'bee_OTU_richness': npy.concatenate([urban_bee_OTU_richness, \\\n",
    "        rural_bee_OTU_richness]),\n",
    "    'building_cover_percentage': npy.concatenate([urban_building_cover, \\\n",
    "        rural_building_cover]),\n",
    "    'bombus_visitations': npy.concatenate([urban_bombus_visitations, \\\n",
    "        rural_bombus_visitations]),\n",
    "    'OTU_richness': npy.concatenate([urban_OTU_richness, \\\n",
    "        rural_OTU_richness]),\n",
    "    'seed_set': npy.concatenate([urban_seed_set, rural_seed_set])\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# Calculate green space percentage for the combined data\n",
    "plots_combined_data['green_space_percentage'] = 100 - \\\n",
    "    plots_combined_data['building_cover_percentage']\n",
    "plots_combined_data['green_space_percentage'] = \\\n",
    "    npy.clip(plots_combined_data['green_space_percentage'], a_min = 0, \\\n",
    "        a_max = None)\n",
    "\n",
    "# Prepare plot data\n",
    "plot_data4 = plots_combined_data.set_index('site_name').reset_index()\n",
    "plot_data4['type'] = npy.where(plot_data4['site_name'].str.startswith('U'), \\\n",
    "    'Urban', 'Rural')\n",
    "\n",
    "# Very boring from this point\n",
    "metrics = ['bee_OTU_richness', 'green_space_percentage', 'bombus_visitations', \\\n",
    "    'OTU_richness', 'seed_set']\n",
    "\n",
    "for metric in metrics:\n",
    "    mpy.figure(figsize = (12, 6))\n",
    "    mpy.bar(plot_data4['site_name'], plot_data4[metric], \\\n",
    "        color = ['#bec1bc' if t \\\n",
    "    == 'Urban' else '#599b19' for t in plot_data4['type']])\n",
    "    \n",
    "    mpy.title(f'Comparison of {metric.replace(\"_\", \" \").title()} by Site')\n",
    "    mpy.xlabel('Site Name')\n",
    "    mpy.ylabel(metric.replace(\"_\", \" \").title())\n",
    "    mpy.xticks(rotation = 45)\n",
    "    mpy.tight_layout()\n",
    "    \n",
    "    mpy.show()\n",
    "\n",
    "# Print the final plot data\n",
    "print(plot_data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall I am pretty happy with how my data simulation turned out. I think effectively going through this section of the assignment 3.5 times helped a lot with my understanding of how sample sizes and variables work at more fundamental levels both in general statistical and programming contexts. Like I got better at R thanks to this assignment.\n",
    "\n",
    "I am happy with a normal distribution for the context of data simulation. Somewhere out there, one of those seeds in the simulation of my nearest up cell of code is exactly like the distribution of the actual data points for the variables to do with the study sites. Splitting into rural and urban sites by the end for visual clarity helped as well, rather than just 18 comparison points. As far as sample size goes, I guess I can repeat this procedure of simulation for data4 however many times and get mean +/- standard deviation of those results, but that doesn't seem particularly useful. If the purpose of this assignment is to replicate the experimental procedure of the report then I do not see how effectively duplicating my simulation would do anything relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis:\n",
    "This is the easy part/third it's mostly just troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# LMM for rural/urban site vs proportion of green land uses\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Make sure to define urban_count and rural_count from the getgo here\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "urban_building_cover = npy.random.normal(loc = 42, scale = 30, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_building_cover = npy.random.normal(loc = 45, scale = 100, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "urban_building_cover = npy.clip(urban_building_cover, a_min = 0, a_max = 100)\n",
    "rural_building_cover = npy.clip(rural_building_cover, a_min = 0, a_max = 100)\n",
    "\n",
    "# Simulate green space percentage\n",
    "green_space_percentage = 100 - npy.concatenate([urban_building_cover, \\\n",
    "    rural_building_cover])\n",
    "\n",
    "# Combine data with urban/rural labels\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'green_space_percentage': green_space_percentage,\n",
    "    'type': ['Urban'] * urban_count + ['Rural'] * rural_count\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# Fit a Linear Mixed Model (LMM)\n",
    "model = smf.mixedlm(\"green_space_percentage ~ type\", data = \\\n",
    "    plots_combined_data, groups = plots_combined_data[\"site_name\"])\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Prepare predictions manually\n",
    "predictions = plots_combined_data.copy()\n",
    "predictions['predicted'] = result.fittedvalues\n",
    "\n",
    "# Create a bar plot of observed vs predicted green space percentage\n",
    "mpy.figure(figsize = (12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['green_space_percentage'], \\\n",
    "    color = '#bec1bc', label = 'Observed green space', alpha = 0.6)\n",
    "mpy.bar(predictions['site_name'], predictions['predicted'], color='#599b19', \\\n",
    "label = 'Predicted green space', alpha = 0.6, edgecolor = '#101010')\n",
    "mpy.title('Observed vs Predicted Green Space Percentage by Site')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Green Space Percentage')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n",
    "\n",
    "# Since MixedLM does not provide direct confidence intervals, you can create \n",
    "# approximate CI using the standard errors of the fixed effects\n",
    "predictions['std_error'] = result.bse[1]  # Assuming 'type' is the second term\n",
    "predictions['lower_ci'] = predictions['predicted'] - 1.96 * \\\n",
    "    predictions['std_error']\n",
    "predictions['upper_ci'] = predictions['predicted'] + 1.96 * \\\n",
    "    predictions['std_error']\n",
    "\n",
    "# Plot observed values and confidence intervals\n",
    "mpy.figure(figsize = (12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['green_space_percentage'], \\\n",
    "    color = '#bec1bc', label = 'Observed green space', alpha = 0.6)\n",
    "mpy.errorbar(predictions['site_name'], predictions['predicted'], \n",
    "             yerr = [predictions['predicted'] - predictions['lower_ci'], \n",
    "                   predictions['upper_ci'] - predictions['predicted']], \n",
    "             fmt = 'o', color = '#101010', label = 'Confidence Interval', \\\n",
    "             capsize = 5)\n",
    "mpy.title('Observed Green Space Percentage with Predicted Values \\\n",
    "    and Confidence Intervals')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Green Space Percentage')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GLM for rural/urban site vs trifolium visitation rates by Bombus spp.\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Make sure to define urban_count and rural_count\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Update to new bombus visitation values\n",
    "urban_bombus_visitations = npy.random.normal(loc = 8, scale = 2, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_bombus_visitations = npy.random.normal(loc = 3, scale = 0.5, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "urban_bombus_visitations = npy.clip(urban_bombus_visitations, \\\n",
    "    a_min = 0, a_max = None)\n",
    "rural_bombus_visitations = npy.clip(rural_bombus_visitations, \\\n",
    "    a_min = 0, a_max = None)\n",
    "\n",
    "# Combine data with urban/rural labels\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'bombus_visitations': npy.concatenate([urban_bombus_visitations, \\\n",
    "        rural_bombus_visitations]),\n",
    "    'type': ['Urban'] * urban_count + ['Rural'] * rural_count\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# Fit a GLM using Poisson family\n",
    "model = smf.glm(\"bombus_visitations ~ type\", data = plots_combined_data, \\\n",
    "    family = smy.families.Poisson())\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Prepare predictions\n",
    "predictions = plots_combined_data.copy()\n",
    "predictions['predicted'] = result.predict(predictions)\n",
    "\n",
    "# Create a bar plot of observed vs predicted bombus visitations\n",
    "mpy.figure(figsize = (12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['bombus_visitations'], \\\n",
    "    color = '#bec1bc', label = 'Observed Bombus Visitations', alpha = 0.6)\n",
    "mpy.bar(predictions['site_name'], predictions['predicted'], \\\n",
    "    color='#599b19', label = 'Predicted Bombus Visitations', alpha = 0.6, \\\n",
    "    edgecolor = '#101010')\n",
    "mpy.title('Observed vs Predicted Bombus Visitations by Site')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Bombus Visitations')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n",
    "\n",
    "# Calculate confidence intervals\n",
    "predictions['lower_ci'] = result.get_prediction(predictions).conf_int()[:, 0]\n",
    "predictions['upper_ci'] = result.get_prediction(predictions).conf_int()[:, 1]\n",
    "\n",
    "# Plot observed values and confidence intervals\n",
    "mpy.figure(figsize = (12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['bombus_visitations'], \\\n",
    "    color = '#bec1bc', label = 'Observed Bombus Visitations', alpha = 0.6)\n",
    "mpy.errorbar(predictions['site_name'], predictions['predicted'], \n",
    "             yerr=[predictions['predicted'] - predictions['lower_ci'], \n",
    "                   predictions['upper_ci'] - predictions['predicted']], \n",
    "             fmt = 'o', color='#101010', label = 'Confidence Interval', \\\n",
    "             capsize = 5)\n",
    "mpy.title('Observed Bombus Visitations with Predicted Values and Confidence Intervals')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Bombus Visitations')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GLMM for rural/urban site vs flying insect species richness, well allegedly\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Make sure to define urban_count and rural_count\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "urban_OTU_richness = npy.random.normal(loc = 67, scale = 10, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_OTU_richness = npy.random.normal(loc = 87, scale = 13, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "urban_OTU_richness = npy.clip(urban_OTU_richness, a_min = 0, a_max = None)\n",
    "rural_OTU_richness = npy.clip(rural_OTU_richness, a_min = 0, a_max = None)\n",
    "\n",
    "# Combine data with urban/rural labels\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'OTU_richness': npy.concatenate([urban_OTU_richness, rural_OTU_richness]),\n",
    "    'type': ['Urban'] * urban_count + ['Rural'] * rural_count\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# I literally cannot do poisson regression with a GLMM in python, at least not\n",
    "# without making my own stuff/custom packages/going into some C code which I am\n",
    "# very much so not ready for. Maybe this is for a reason and poisson regression\n",
    "# for a generalized linear mixed model is not ideal, or maybe R is just the\n",
    "# superior programming language for this specific instance\n",
    "model = smf.glm(\"OTU_richness ~ type\", data = plots_combined_data, family = \\\n",
    "    smy.families.Poisson())\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Prepare predictions\n",
    "predictions = plots_combined_data.copy()\n",
    "predictions['predicted'] = result.predict(predictions)\n",
    "\n",
    "# Create a bar plot of observed vs predicted OTU richness\n",
    "mpy.figure(figsize = (12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['OTU_richness'], \\\n",
    "    color = '#bec1bc', label = 'Observed OTU Richness', alpha = 0.6)\n",
    "mpy.bar(predictions['site_name'], predictions['predicted'], \\\n",
    "    color = '#599b19', label = 'Predicted OTU Richness', alpha = 0.6, \\\n",
    "    edgecolor='#101010')\n",
    "mpy.title('Observed vs Predicted OTU Richness by Site')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('OTU Richness')\n",
    "mpy.xticks(rotation=45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n",
    "\n",
    "# Calculate confidence intervals\n",
    "predictions['lower_ci'] = result.get_prediction(predictions).conf_int()[:, 0]\n",
    "predictions['upper_ci'] = result.get_prediction(predictions).conf_int()[:, 1]\n",
    "\n",
    "# Plot observed values and confidence intervals\n",
    "mpy.figure(figsize = (12, 8))\n",
    "mpy.bar(predictions['site_name'], predictions['OTU_richness'], \\\n",
    "    color = '#bec1bc', label = 'Observed OTU Richness', alpha = 0.6)\n",
    "mpy.errorbar(predictions['site_name'], predictions['predicted'], \n",
    "             yerr = [predictions['predicted'] - predictions['lower_ci'], \n",
    "                   predictions['upper_ci'] - predictions['predicted']], \n",
    "             fmt = 'o', color = '#101010', label = 'Confidence Interval', \\\n",
    "                capsize = 5)\n",
    "mpy.title('Observed OTU Richness with Predicted Values and Confidence Intervals')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('OTU Richness')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GLM for rural/urban site vs bee species richness\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Make sure to define urban_count and rural_count\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Update to new bee OTU richness values\n",
    "urban_bee_OTU_richness = npy.random.normal(loc = 9, scale = 4, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_bee_OTU_richness = npy.random.normal(loc = 5.5, scale = 3, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "urban_bee_OTU_richness = npy.clip(urban_bee_OTU_richness, a_min = 0, \\\n",
    "    a_max = None)\n",
    "rural_bee_OTU_richness = npy.clip(rural_bee_OTU_richness, a_min = 0, \\\n",
    "    a_max = None)\n",
    "\n",
    "# Combine data with urban/rural labels\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'bee_OTU_richness': npy.concatenate([urban_bee_OTU_richness, \\\n",
    "        rural_bee_OTU_richness]),\n",
    "    'type': ['Urban'] * urban_count + ['Rural'] * rural_count\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# Fit a GLM using Poisson family\n",
    "model = smf.glm(\"bee_OTU_richness ~ type\", data = plots_combined_data, \\\n",
    "    family = smy.families.Poisson())\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Prepare predictions\n",
    "predictions = plots_combined_data.copy()\n",
    "predictions['predicted'] = result.predict(predictions)\n",
    "\n",
    "# Create a bar plot of observed vs predicted bee OTU richness\n",
    "mpy.figure(figsize=(12, 6))\n",
    "mpy.bar(predictions['site_name'], predictions['bee_OTU_richness'], \\\n",
    "    color='#bec1bc', label='Observed Bee OTU Richness', alpha=0.6)\n",
    "mpy.bar(predictions['site_name'], predictions['predicted'], \\\n",
    "    color = '#599b19', label='Predicted Bee OTU Richness', \\\n",
    "    alpha = 0.6, edgecolor = '#101010')\n",
    "mpy.title('Observed vs Predicted Bee OTU Richness by Site')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Bee OTU Richness')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n",
    "\n",
    "# Calculate confidence intervals\n",
    "predictions['lower_ci'] = result.get_prediction(predictions).conf_int()[:, 0]\n",
    "predictions['upper_ci'] = result.get_prediction(predictions).conf_int()[:, 1]\n",
    "\n",
    "mpy.figure(figsize = (12, 8))\n",
    "mpy.bar(predictions['site_name'], predictions['bee_OTU_richness'], \\\n",
    "    color = '#bec1bc', label = 'Observed Bee OTU Richness', alpha = 0.6)\n",
    "mpy.errorbar(predictions['site_name'], predictions['predicted'], \n",
    "             yerr=[predictions['predicted'] - predictions['lower_ci'], \n",
    "                   predictions['upper_ci'] - predictions['predicted']], \n",
    "             fmt = 'o', color = '#101010', label = 'Confidence Interval', \\\n",
    "                capsize = 5)\n",
    "mpy.title('Observed Bee OTU Richness with Predicted Values and Confidence Intervals')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Bee OTU Richness')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# LMM of urban/rural site vs seed set\n",
    "\n",
    "import matplotlib.pyplot as mpy\n",
    "import pandas as pdy\n",
    "import numpy as npy\n",
    "import statsmodels.api as smy\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "npy.random.seed(14)\n",
    "\n",
    "# Define urban_count and rural_count\n",
    "urban_count = 9\n",
    "rural_count = 9\n",
    "\n",
    "data4 = {\n",
    "    'site_name': [\n",
    "        'U_Halle', 'U_Leipzig', 'U_Jena', 'U_Dresden', 'U_Chemnitz',\n",
    "        'U_Braunschweig', 'U_Potsdam', 'U_Berlin', 'U_Gottingen',\n",
    "        'R_Halle', 'R_Leipzig', 'R_Jena', 'R_Dresden', 'R_Chemnitz',\n",
    "        'R_Braunschweig', 'R_PotsDAM', 'R_BERLIN', 'R_Gottingen'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Update to new seed set values\n",
    "urban_seed_set = npy.random.normal(loc = 230, scale = 30, \\\n",
    "    size = urban_count).round().astype(int)\n",
    "rural_seed_set = npy.random.normal(loc = 163, scale = 30, \\\n",
    "    size = rural_count).round().astype(int)\n",
    "\n",
    "urban_seed_set = npy.clip(urban_seed_set, a_min = 0, a_max=None)\n",
    "rural_seed_set = npy.clip(rural_seed_set, a_min = 0, a_max=None)\n",
    "\n",
    "# Combine data with urban/rural labels\n",
    "combined_data = {\n",
    "    'site_name': data4['site_name'],\n",
    "    'seed_set': npy.concatenate([urban_seed_set, rural_seed_set]),\n",
    "    'type': ['Urban'] * urban_count + ['Rural'] * rural_count\n",
    "}\n",
    "\n",
    "plots_combined_data = pdy.DataFrame(combined_data)\n",
    "\n",
    "# Fit a LMM\n",
    "model = smf.mixedlm(\"seed_set ~ type\", plots_combined_data, \\\n",
    "    groups = plots_combined_data[\"site_name\"])\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "\n",
    "# Prepare predictions\n",
    "predictions = plots_combined_data.copy()\n",
    "predictions['predicted'] = result.predict(predictions)\n",
    "\n",
    "# Create a bar plot of observed vs predicted seed set\n",
    "mpy.figure(figsize = (12, 8))\n",
    "mpy.bar(predictions['site_name'], predictions['seed_set'], \\\n",
    "    color = '#bec1bc', label = 'Observed Seed Set', alpha = 0.6)\n",
    "mpy.bar(predictions['site_name'], predictions['predicted'], color='#599b19', \\\n",
    "label='Predicted Seed Set', alpha = 0.6, edgecolor = '#101010')\n",
    "mpy.title('Observed vs Predicted Seed Set by Site')\n",
    "mpy.xlabel('Site Name')\n",
    "mpy.ylabel('Seed Set')\n",
    "mpy.xticks(rotation = 45)\n",
    "mpy.legend()\n",
    "mpy.tight_layout()\n",
    "mpy.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5:\n",
    "So getting the semplot library turned out to take quite some work for me to set up in python on my computer, and by that I mean it has been 30 hours no sleep and still no end in sight. Luckily for me, the semplot package in R was also quite different to set up on either my laptop or desktop, and these two combined are what has taken \n",
    "\n",
    "With that said, I will sort this out and have my final assignment for this class explicitly included code for a simulated structural equation model in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# My fifth model done in R because I could not figure out how to download one\n",
    "# library on linux within a reasonable timespan. Anyways here are the packages\n",
    "# need for this simulated structural equation model. Repeating simulations for R\n",
    "library(dplyr)\n",
    "library(lavaan)\n",
    "library(semPlot)\n",
    "\n",
    "set.seed(14)\n",
    "# I am indeed aware this seed will give different data since it is not in python\n",
    "# but consistency doesn't hurt.\n",
    "\n",
    "# Starting with the 9 pairs organization first this time, continuing the good\n",
    "# habit\n",
    "n_sites <- 9\n",
    "\n",
    "# Simulating the data but with the context of the n9 sites for each category\n",
    "# already makes for so much more elegant code.\n",
    "n_sites <- 9\n",
    "urban_data <- data.frame(\n",
    "  lnd = rep(\"urban\", n_sites),\n",
    "  b_O = rnorm(n_sites, mean = 9, sd = 4),\n",
    "  bm_ = rnorm(n_sites, mean = 8, sd = 2),\n",
    "  OTU = rnorm(n_sites, mean = 67, sd = 10)\n",
    ")\n",
    "rural_data <- data.frame(\n",
    "  lnd = rep(\"rural\", n_sites),\n",
    "  b_O = rnorm(n_sites, mean = 5.5, sd = 3),\n",
    "  bm_ = rnorm(n_sites, mean = 3, sd = 0.5),\n",
    "  OTU = rnorm(n_sites, mean = 87, sd = 13)\n",
    ")\n",
    "total_data <- bind_rows(urban_data, rural_data)\n",
    "\n",
    "# Indentation in R makes me uncomfortable :(\n",
    "total_data <- total_data %>%\n",
    "  mutate(sd_ = ifelse(lnd == \"urban\",\n",
    "                      rnorm(n_sites, mean = 230, sd = 30),\n",
    "                      rnorm(n_sites, mean = 163, sd = 30)))\n",
    "\n",
    "# These give us very fun simulations to look at, well I think it's fun because\n",
    "# it's different that the simulations in python but also not really.\n",
    "print(\"First rounds of simulated data:\")\n",
    "head(total_data)\n",
    "print(\"\\nSummary statistics of the simulated data:\")\n",
    "summary(total_data)\n",
    "print(\"\\nStructure of the dataset:\")\n",
    "str(total_data)\n",
    "\n",
    "# Honestly I am still confused what exactly this chunk of code is doing, why\n",
    "# there has to be a singular quotation and the indentations and aaaa\n",
    "# All I know is if I try to change around the formatting it breaks\n",
    "model <- '\n",
    "  # Structural model\n",
    "  sd_ ~ b_O + bm_ + OTU\n",
    "  b_O ~ lnd\n",
    "  bm_ ~ lnd\n",
    "  OTU ~ lnd\n",
    "'\n",
    "\n",
    "# Now for the actual SEM, this part was fine on python for me it was just the\n",
    "# figure that was... very sad. But yes it is cool how much briefer the total\n",
    "# amount of text for statistical analyses can be like with the very few words\n",
    "# needed here to set up the SEM and its numeric output.\n",
    "fit <- sem(model, data = total_data)\n",
    "print(\"\\nModel Fit Summary:\")\n",
    "summary(fit, fit.measures = TRUE, standardized = TRUE)\n",
    "\n",
    "# Bonus content/data output because I already ended up a day late anyways:\n",
    "print(\"\\nStandardized Parameter Estimates:\")\n",
    "standardizedSolution(fit)\n",
    "print(\"\\nAdditional Fit Measures:\")\n",
    "fitMeasures(fit, c(\"cfi\", \"tli\", \"rmsea\", \"srmr\"))\n",
    "var_order <- lavNames(fit)\n",
    "\n",
    "# This upcoming section is really weird and I am still iffy on some of its\n",
    "# components for sure but it was the best way for me to get an SEM diagram\n",
    "# visualization with the restrictions of the simulations and everything being in\n",
    "# a given programming language. Apparently there are some scientific articles\n",
    "# specifically to do with the semPlot package that are extremely impressive to\n",
    "# look at and some additions like psychonetrics that are much more advanced,\n",
    "# but based on the instructions \"You will learn much more, and accomplish much \n",
    "# more if you do this assignment from the text of the paper only.\" I just kinda\n",
    "# messed around and got the subsequent monstrosity.\n",
    "\n",
    "# A matrix with specific labels based on what I saw in other trials of the SEM\n",
    "# diagrams, they are 3 characters long each and I just took the 3 characters\n",
    "# that... some voodo forces of stack overflow and claude agreed upon. The figure\n",
    "# will just break if the labels go above 3. This assignment was already enough\n",
    "# as is so I went against also looking into how to adjust the text size and how\n",
    "# large each \"SEM variable container\" was. That is definitely a priority for\n",
    "# the final assignment however.\n",
    "layout_mat <- matrix(0, nrow = length(var_order), ncol = 2)\n",
    "rownames(layout_mat) <- var_order\n",
    "layout_mat[\"lnd\",] <- c(-2, 0)    # Left\n",
    "layout_mat[\"b_O\",] <- c(0, 1)     # Top\n",
    "layout_mat[\"bm_\",] <- c(0, -1)    # Bottom\n",
    "layout_mat[\"OTU\",] <- c(0, 0)     # Middle\n",
    "layout_mat[\"sd_\",] <- c(2, 0)     # Right\n",
    "# These coordinates are there to give me a slightly costumized layout and not\n",
    "# some stupid looking off-center triangle or a diamond where variable\n",
    "# associations cut through each other in very displeasing ways.\n",
    "\n",
    "semPaths(fit,\n",
    "         what = \"std\",\n",
    "         layout = layout_mat,\n",
    "         edge.label.cex = 0.8,\n",
    "         node.label.cex = 1.2,\n",
    "         residuals = TRUE,\n",
    "         fade = FALSE,\n",
    "         sizeMan = 8,\n",
    "         sizeLat = 8,\n",
    "         label.cex = 1.2,\n",
    "         label.scale = FALSE,\n",
    "         title = FALSE,\n",
    "         edge.color = \"#5b4b11\",\n",
    "         node.color = \"#c4d787\",\n",
    "         title.colour = \"#a9b1ac\",\n",
    "         layout.algorithm = \"manual\")  # Force manual layout\n",
    "# Relatively speaking I am happy with the layout of this figure, the landscape\n",
    "# and seed_distribution variables are separated from each other in a way that is\n",
    "# intuitive and can explain maybe an expected effect from the context of this\n",
    "# experiment design.\n",
    "\n",
    "# More extra statistical tests. I am not too concerned about the other analyses\n",
    "# of this report of mine, but since SEM is something I am fairly underqualified\n",
    "# in relative to how important of an experiment design/analysis tool it is for\n",
    "# my current research project, more stuff like this doesn't hurt. Honestly I am\n",
    "# unsure what you as an instructor/grader prioritize with the \"proper use of\n",
    "# statistical tests\", like if there are too many with this SEM simulation or\n",
    "# too few elsewhere. Don't mind losing marks in that category, this assignment\n",
    "# became way more about messing around with what goes into reverse engineering\n",
    "# data and the different upsides and downsides of python and R as programming\n",
    "# tools in this specific context.\n",
    "print(\"\\nModification Indices:\")\n",
    "modificationindices(fit)\n",
    "print(\"\\nR-squared values for endogenous variables:\")\n",
    "inspect(fit, \"r2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Overall thoughts from my end:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the context of the original study, there was only so much I could do in a reasonable time frame, and even trying to \"restrict my hours\" ended up being horrifically off the mark. In some senses it really is not surprising to find that if there is a solid statistical foundation for the rest of anlyses in an experiment, in this case the paired study sites, it is pretty easy to replicate linear models and the like. I am really glad I went through and was very overboard with the data simulation and firstly replication. It helped me with understanding the value of overall trends and concepts at both a statistical and biological level over samples. I think in field ecology it can be pretty easy to tunnel-vision in on samples (relatively to stuff like physics or biochem anyways), but even here with practically the answers right in front of me I would say the second round of data simulations/replications I did showed a better understanding of topics and especially statistical programming than the first.\n",
    "\n",
    "The structural equation model is not too enticing or mind-blowing, this was more its own form of practice for me. Conceptually they are nothing too crazy to process, but only really are whorthwhile in the context of already complex study designs. Also pretty hard to make pretty in R apparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly I put myself through an unnessary amount of suffering for this assignment all to hand it in a day late anyways. Both my laptop and desktop run on linux which sometimes ends up in situations where certain programming packages don't like to play, and in the interest of saving time I could have started a windows virtual machine on my desktop and had the packages easily available for me, but this was more helpful in the long run. Screwing myself over and needing to use R for the last stretch of this assignment unless I wanted to hand in two days late brute forced me to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
